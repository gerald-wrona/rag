{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c847653-41e4-4828-bdf6-03a78b4760d5",
   "metadata": {},
   "source": [
    "#### 80 or five"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d26df-51ee-4580-8425-9b092832668f",
   "metadata": {},
   "source": [
    "The course opens by saying they have 80 document loaders, but the docs show these five below. All follow a standard structure, but YouTube conversions have the greatest number of dependencies. The YouTube conversion downloads the video to your local as an m4a. Even though Macbeth is only 212 pages it took ages longer than the others to run. \n",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fff7994-7525-47b1-87bf-63198f31cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33d4142-7740-49d2-a36e-b15a89d8dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"macbeth.pdf\")\n",
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d72fb15-76b2-4416-a807-2c874816ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macbeth\n",
    "page_count = len(pdf_docs)\n",
    "example_page = pdf_docs[40]\n",
    "example_text = example_page.page_content[0:500]\n",
    "example_metadata = example_page.metadata #{'source': 'macbeth.pdf', 'page': 40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098c039e-997b-416a-a41f-e957150baf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/TRrr-8mWoU0?si=nM2Qj2-_KMwg9abH\n",
      "[youtube] TRrr-8mWoU0: Downloading webpage\n",
      "[youtube] TRrr-8mWoU0: Downloading ios player API JSON\n",
      "[youtube] TRrr-8mWoU0: Downloading android player API JSON\n",
      "[youtube] TRrr-8mWoU0: Downloading m3u8 information\n",
      "[info] TRrr-8mWoU0: Downloading 1 format(s): 140\n",
      "[download] C:\\Users\\wrona\\Documents\\upskill\\RAG\\langchain\\why should people care about mac demarco.m4a has already been downloaded\n",
      "[download] 100% of  170.63KiB\n",
      "[ExtractAudio] Not converting audio C:\\Users\\wrona\\Documents\\upskill\\RAG\\langchain\\why should people care about mac demarco.m4a; file is already in target format m4a\n",
      "Transcribing part 1!\n"
     ]
    }
   ],
   "source": [
    "# Load Youtube\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "video_address = \"https://youtu.be/TRrr-8mWoU0?si=nM2Qj2-_KMwg9abH\"\n",
    "write_vid_here = \"C:\\\\Users\\\\wrona\\\\Documents\\\\upskill\\\\RAG\\\\langchain\\\\\"\n",
    "vid_loader = GenericLoader(YoutubeAudioLoader([video_address], write_vid_here),\n",
    "                       OpenAIWhisperParser())\n",
    "yt_docs = vid_loader.load()\n",
    "yt_transcribed = yt_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb54cdd-865d-4ccc-9eab-974db39a55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any website\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "web_address = \"https://supermacs.ie/supermacs-food/\"\n",
    "web_loader = WebBaseLoader(url)\n",
    "web_docs = web_loader.load()\n",
    "first_page_content = web_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d1ea0d7-0a4a-43d2-bec3-1a06d37d43c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item_name: Apples, McIntosh    \\n Base year 1982=100\\nperiod2: Jan\\nTextbox214: % chg.\\nJAN Ãª\\nstat_year: 2023\\ntextbox2: 233.7\\nfootnote_code: \\npctChg3: 3.00%\\nTextbox206: WPU01110216 Apples, McIntosh'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any CSV; but it's pretty strict about the format\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "csv_loader = CSVLoader('PPI.csv')\n",
    "csv_docs = csv_loader.load()\n",
    "first_csv_doc_content = csv_docs[0].page_content\n",
    "first_csv_doc_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153863d-ce7b-48a5-9f98-0ce7c3cae12b",
   "metadata": {},
   "source": [
    "#### Langchunk\n",
    "Ideal chunks are semantically complete, meaning that in a loving two-page description of McIntosh apples a single chunk would capture the portion dedicated to flavor, maybe a whole paragraph. If the passionate author instead divided their treatise on flavour into two paragraphs, one dedicated to sweetness and another to tartness, then ideally we'll have one chunk for each. If this monumental thinker went on to a write a third paragraph contrasting the virtues of sweetness and tartness, praising the divine balance which McIntosh achieves, that would be a third chunk, but given a question of flavor it might also be relevantly retrieved."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8db2858d-a5f6-4324-8469-ac87b0a4c9a5",
   "metadata": {},
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f179a3d-20ea-465a-bc4e-d6a7328e1219",
   "metadata": {},
   "source": [
    "Above we see we can change the length function, so we can define that if the default isn't providing the result we want. These chunk parameters may also need fine-tuning. The docs show examples for split by token, split by character and recursive split by character. https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94f206-aea6-4c0a-a886-28a88fc5e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
